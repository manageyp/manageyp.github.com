---
layout: post
title: Chinese Word Segmentation Basic Concepts
description: Chinese Word Segmentation Basic Concepts
categories: Python
tags: Python
record_on: 2018-06-26
---

<p>
中文分词总结之基本概念讲解篇。
</p>

<h3>什么是中文分词</h3>
<p>
  首先，分词（Word Segmentation）是指将连续的字序列，按照一定的规则，重新组合成词序列的过程。<br>

  而中文分词（Chinese Word Segmentation），是指将一个汉字序列进行切分，得到一个个单独的词。
</p>

<h3>中文分词与英文分词的区别</h3>
<p>
  中文分词与英文分词有很大的不同。<br>

  以英文为代表的拉丁语系语言，以空格作为天然的分隔符。每一个单词就是一个词。<br>
  计算机可以简单的按照空格和标点符号，将其分为独立的词。
  并且，每个词的含义也基本确定。<br>
  <pre class="prettyprint lang-html">
    原始文本：Hello, Henry. Good morning.
    分词结果：Hello  ,  Henry  .  Good  morning  .
  </pre>

  而中文是以汉字为基本的单位，词语之间没有明显的区分标记。现代汉语中双字和多字词居多，一个字不再等同于一个词。<br>
  让计算机将其切分为独立的有意义的词，并且，切分后的结果，要保留原始语句的含义，比较困难。<br>
  <pre class="prettyprint lang-html">
    原始文本：早上好亨利，我们一起去吃早餐吧。
    分词结果：早上  好  亨利  ，  我们  一起  去  吃  早餐  吧  。
  </pre>
</p>

<h3>中文分词方法和算法</h3>
<p>
  为了让计算机能够对中文进行分词，我们引入一定的分词策略和算法，将待分析的中文语句进行正确的处理。
  以下，列举四大类分词方法，每种分词方法中包含多种分词的算法。<br>

  <ul>
    <li style="list-style: none;"><b>一、基于规则的分词方法</b></li>
    <li style="list-style: none;"><b>二、基于统计的分词方法</b></li>
    <li style="list-style: none;"><b>三、基于理解的分词方法</b></li>
  </ul>
</p>

<h4>一、基于规则的分词方法</h4>
<p>
  基于规则的分词方法，又称为机械分词方法。该方法是按照一定的策略将待分析的中文文本，与一个“充分大的”机器词典中的词进行匹配。若在词典中找到某个字符串，则匹配成功，识别出该词。<br>
  因此，该方法也叫字符串匹配分词。
  该方法有三个要素：分词词典、文本扫描顺序和匹配原则。
  其中，文本的扫描顺序有正向、逆向和双向。
  匹配原则主要有最大匹配、最小匹配、逐词匹配和最佳匹配。
</p>
<p>
  <b>1.1 规则分词方法的特点</b><br>
  逆向匹配的切分精度略高与正向匹配，遇到歧义现象也较少。<br>
  统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。这样的精度远远不能满足实际的需要。<br>
  实际使用的分词系统，都是把该分词方法作为一种初分手段，还需要通过利用各种其他的语言信息，来进一步提高分词的准确率。
</p>
<p>
  <b>1.2 规则分词方法的改进</b><br>
  一种方法是改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词为断点，可将原字符串分为较小的串，再来进行分词，从而减少匹配的错误率<br>
  另一种方法是将分词和词类标志结合起来，利用丰富的词类信息，对分词决策提供帮助。并且在标注过程中又反过来对分词结果进行校验、调整，从而极大的提供切分的准确率。
</p>

<h4>二、基于统计的分词方法</h4>
<p>
  统计分词的主要思想：词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此，可以对文本中相邻出现的各个字的组合，出现的频率进行统计。相邻两个汉字的互现信息，体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为该字的组合可能构成了一个词。<br>
  该方法只需对文本中的字组合频率进行统计，因而也叫统计取词方法。
</p>
<p>
  <b>2.1 统计分词方法的特点</b><br>
  统计分词有一定的局限性，会出现一些共线频率高，但是，不是组合词的情况。例如：“之一”、“有的”、“我的”等。
  并且对常用词的识别精度差，统计过程耗时。<br>
  实际应用中，会使用常用词词典进行串匹配分词，同时使用统计方法识别一些新的词。
  既可以发挥匹配分词切分速度快、效率高的特点，又结合上下文识别生词、自动消除歧义的优点。<br>
  该分词方法应用的统计模型有：N 元文法模型（N-gram）、隐马尔可夫（Hiden Markov Model, HMM）、最大熵模型(ME)、条件随机场模型(Conditional Random Fields, CRF)等。
</p>
<p>
  <b>2.2 统计分词方法的改进</b><br>
  基于统计机器学习的方法。首先给出大量已经分词的文本，利用统计机器学习模型，学习词语切分的规律（称为训练），从而实现对未知文本的切分。<br>
  这种方法充分利用了汉语组词的规律，进行分词。缺点是需要有大量预先分好词的文本库作为支撑，同时，训练过程中的时空开销很大。
</p>

<h4>三、基于理解的分词方法</h4>
<p>
  通过让计算机模拟人对句子的理解，达到识别词的效果。<br>
  基本思想是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。<br>
  通常包括三个部分：分词系统、句法语义系统和总控系统。
</p>
<p>
  <b>3.1 理解分词方法的特点</b><br>
  在总控系统的协调下，分词子系统可以获得有关词、句子等的句法和语义信息，从而对分词歧义进行判断，即模拟了人对句子的理解过程。<br>
  这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复制性，难以将各种语言信息组织成机器可以直接读取的形式，因此，目前基于理解的分词系统还处于实验
</p>





<h3>参考链接</h3>
<p>
  <a href="https://www.jianshu.com/p/e978053b0b95" target="_blank">中文分词算法总结</a></br>
  <a href="https://cuiqingcai.com/5844.html" target="_blank">中文分词原理及工具</a></br>
</p>
